# ðŸŒ¦ï¸ OpenWeather Data Pipeline

![Analyses](assets/weather_analysis.png)

## Overview

This project automates the ingestion, transformation, and cataloging of weather data retrieved from the **OpenWeather API**, using a serverless data pipeline on AWS. The processed data is made available for querying through **Amazon Athena**.

## Architecture

![Architecture](assets/OpenWeatherArchitecture.png)

The solution is built using the following AWS components:

- **EventBridge** â€” Triggers the pipeline every **Monday at 12 PM (UTC)**.
- **Step Functions** â€” Orchestrates the workflow:
  1. **Glue Ingest Job**: Fetches weather data from the OpenWeather API and stores it in the Raw S3 bucket.
  2. **Glue Transform Job**: Cleans/transforms the data and stores it in the Processed S3 bucket.
  3. **Glue Crawler**: Scans the processed bucket and updates the Glue Catalog.
- **S3 Buckets**:
  - `weather-raw-*`: Stores raw weather data.
  - `weather-processed-*`: Stores transformed data.
  - `weather-glue-scripts-*`: Contains Glue ETL scripts.
  - `weather-states-*`: Backend and temporary storage.
- **AWS Glue Catalog** â€” Holds metadata for querying the processed data using Athena.
- **Amazon Athena** â€” Enables SQL queries over processed data stored in S3.

---

## Features

- ðŸ›°ï¸ Fully serverless, event-driven architecture  
- ðŸ“… Scheduled execution using EventBridge  
- ðŸ§ª Data cataloging with Glue Crawlers  
- ðŸ“Š Query-ready data in Athena  
- âš™ï¸ Infrastructure as Code with Terraform  
- âœ… GitHub Actions for CI/CD  

---

## Repository Structure

```bash
.
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ OpenWeatherArchitecture.png      # Architecture diagram
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ s3.tf                            # S3 buckets and policies
â”‚   â”œâ”€â”€ iam.tf                           # IAM roles and policies
â”‚   â”œâ”€â”€ glue_ingest.tf                   # Glue job for ingestion
â”‚   â”œâ”€â”€ glue_transform.tf                # Glue job for transformation
â”‚   â”œâ”€â”€ glue_database.tf                 # Glue catalog database
â”‚   â”œâ”€â”€ crawler.tf                       # Glue crawler for processed data
â”‚   â”œâ”€â”€ step_functions.tf                # Step Function workflow
â”‚   â”œâ”€â”€ eventbridge.tf                   # EventBridge rule
â”œâ”€â”€ src/                                 # Glue scripts (ingest.py, transform.py)
â”œâ”€â”€ main.tf                              # Terraform root module
â”œâ”€â”€ .github/workflows/deploy.yml         # CI/CD pipeline for Terraform
â””â”€â”€ README.md

## ðŸ› ï¸ Deployment

This project uses **Terraform** to provision infrastructure and **GitHub Actions** for CI/CD.

### âš™ï¸ Terraform Workflow

When a pull request is opened to the `main` branch (or the workflow is manually triggered), the pipeline will:

1. Initialize Terraform
2. Import existing resources
3. Validate the configuration
4. Plan and apply the changes

### ðŸš€ Manual Trigger

You can also **manually trigger** the workflow from GitHub Actions with an option to destroy the resources using the `workflow_dispatch` input.

### âœ… Requirements

- AWS Account
- Terraform >= **1.5.7**
- GitHub Actions Secrets:
  - `AWS_ACCESS_KEY_ID`
  - `AWS_SECRET_ACCESS_KEY`
  - `API_KEY_API_WEATHER` (for OpenWeather API)

### ðŸ“Œ Notes

- Imported resources like **S3 buckets** and **IAM roles** are already created **outside** Terraform. They are imported during deployment to avoid recreation.
- The use of the `terraform import` command inside the pipeline is **required** because Terraform does not manage pre-existing resources unless explicitly imported.
- Although the Terraform **backend** is defined in `main.tf`, the **initial state management** is bootstrapped via CI p